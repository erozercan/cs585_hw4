Training Logs=

  0%|          | 0/50 [00:00<?, ?it/s]Epoch [1/50], Step [10/24], Loss: 3.0580
Epoch [1/50], Step [20/24], Loss: 1.7837
<ipython-input-11-7392458b459d>:34: RuntimeWarning: invalid value encountered in divide
  class_iou = np.nan_to_num(class_intersection / class_union)
  2%|▏         | 1/50 [04:14<3:27:46, 254.41s/it]Pixel accuracy: 0.6062, Mean IoU: 0.0622, Frequency weighted IoU: 0.4238, Loss: 1.4708
Epoch [2/50], Step [10/24], Loss: 1.3009
Epoch [2/50], Step [20/24], Loss: 1.1561
  4%|▍         | 2/50 [07:32<2:56:51, 221.08s/it]Pixel accuracy: 0.6509, Mean IoU: 0.0752, Frequency weighted IoU: 0.4728, Loss: 1.1994
Epoch [3/50], Step [10/24], Loss: 1.0501
Epoch [3/50], Step [20/24], Loss: 0.9471
  6%|▌         | 3/50 [10:48<2:44:19, 209.78s/it]Pixel accuracy: 0.7225, Mean IoU: 0.0961, Frequency weighted IoU: 0.5525, Loss: 1.0150
Epoch [4/50], Step [10/24], Loss: 0.8834
Epoch [4/50], Step [20/24], Loss: 0.8730
  8%|▊         | 4/50 [14:07<2:37:37, 205.59s/it]Pixel accuracy: 0.7623, Mean IoU: 0.1246, Frequency weighted IoU: 0.6189, Loss: 0.8721
Epoch [5/50], Step [10/24], Loss: 0.7670
Epoch [5/50], Step [20/24], Loss: 0.7250
 10%|█         | 5/50 [17:21<2:31:03, 201.41s/it]Pixel accuracy: 0.7932, Mean IoU: 0.1539, Frequency weighted IoU: 0.6595, Loss: 0.7215
Epoch [6/50], Step [10/24], Loss: 0.7303
Epoch [6/50], Step [20/24], Loss: 0.7124
 12%|█▏        | 6/50 [20:42<2:27:39, 201.36s/it]Pixel accuracy: 0.7848, Mean IoU: 0.1483, Frequency weighted IoU: 0.6486, Loss: 0.7674
Epoch [7/50], Step [10/24], Loss: 0.6247
Epoch [7/50], Step [20/24], Loss: 0.6010
 14%|█▍        | 7/50 [23:55<2:22:20, 198.62s/it]Pixel accuracy: 0.7936, Mean IoU: 0.1714, Frequency weighted IoU: 0.6840, Loss: 0.7362
Epoch [8/50], Step [10/24], Loss: 0.6187
Epoch [8/50], Step [20/24], Loss: 0.5863
 16%|█▌        | 8/50 [27:07<2:17:21, 196.23s/it]Pixel accuracy: 0.8144, Mean IoU: 0.1683, Frequency weighted IoU: 0.6892, Loss: 0.6649
Epoch [9/50], Step [10/24], Loss: 0.5869
Epoch [9/50], Step [20/24], Loss: 0.5611
 18%|█▊        | 9/50 [30:17<2:12:51, 194.43s/it]Pixel accuracy: 0.8332, Mean IoU: 0.1890, Frequency weighted IoU: 0.7154, Loss: 0.5853
Epoch [10/50], Step [10/24], Loss: 0.5384
Epoch [10/50], Step [20/24], Loss: 0.5242
 20%|██        | 10/50 [33:29<2:09:02, 193.55s/it]Pixel accuracy: 0.8335, Mean IoU: 0.2129, Frequency weighted IoU: 0.7243, Loss: 0.5840
Epoch [11/50], Step [10/24], Loss: 0.5621
Epoch [11/50], Step [20/24], Loss: 0.5247
 22%|██▏       | 11/50 [36:40<2:05:25, 192.97s/it]Pixel accuracy: 0.7211, Mean IoU: 0.1700, Frequency weighted IoU: 0.6246, Loss: 0.9709
Epoch [12/50], Step [10/24], Loss: 0.8964
Epoch [12/50], Step [20/24], Loss: 0.7842
 24%|██▍       | 12/50 [39:51<2:01:48, 192.33s/it]Pixel accuracy: 0.7958, Mean IoU: 0.1787, Frequency weighted IoU: 0.6685, Loss: 0.7340
Epoch [13/50], Step [10/24], Loss: 0.6466
Epoch [13/50], Step [20/24], Loss: 0.5574
 26%|██▌       | 13/50 [43:06<1:59:07, 193.17s/it]Pixel accuracy: 0.8334, Mean IoU: 0.2320, Frequency weighted IoU: 0.7288, Loss: 0.5867
Epoch [14/50], Step [10/24], Loss: 0.5345
Epoch [14/50], Step [20/24], Loss: 0.4867
 28%|██▊       | 14/50 [46:17<1:55:25, 192.38s/it]Pixel accuracy: 0.8428, Mean IoU: 0.2395, Frequency weighted IoU: 0.7386, Loss: 0.5646
Epoch [15/50], Step [10/24], Loss: 0.4669
Epoch [15/50], Step [20/24], Loss: 0.4820
 30%|███       | 15/50 [49:28<1:52:01, 192.03s/it]Pixel accuracy: 0.8497, Mean IoU: 0.2538, Frequency weighted IoU: 0.7555, Loss: 0.5322
Epoch [16/50], Step [10/24], Loss: 0.4611
Epoch [16/50], Step [20/24], Loss: 0.4462
 32%|███▏      | 16/50 [52:40<1:48:51, 192.10s/it]Pixel accuracy: 0.8427, Mean IoU: 0.2543, Frequency weighted IoU: 0.7388, Loss: 0.5315
Epoch [17/50], Step [10/24], Loss: 0.4520
Epoch [17/50], Step [20/24], Loss: 0.4275
 34%|███▍      | 17/50 [55:53<1:45:43, 192.22s/it]Pixel accuracy: 0.8530, Mean IoU: 0.2701, Frequency weighted IoU: 0.7618, Loss: 0.5075
Epoch [18/50], Step [10/24], Loss: 0.4650
Epoch [18/50], Step [20/24], Loss: 0.4284
 36%|███▌      | 18/50 [59:07<1:42:51, 192.85s/it]Pixel accuracy: 0.8522, Mean IoU: 0.2640, Frequency weighted IoU: 0.7555, Loss: 0.5267
Epoch [19/50], Step [10/24], Loss: 0.4193
Epoch [19/50], Step [20/24], Loss: 0.3992
 38%|███▊      | 19/50 [1:02:17<1:39:08, 191.89s/it]Pixel accuracy: 0.8616, Mean IoU: 0.2880, Frequency weighted IoU: 0.7695, Loss: 0.4814
Epoch [20/50], Step [10/24], Loss: 0.3808
Epoch [20/50], Step [20/24], Loss: 0.3799
 40%|████      | 20/50 [1:05:29<1:36:02, 192.09s/it]Pixel accuracy: 0.8595, Mean IoU: 0.2851, Frequency weighted IoU: 0.7678, Loss: 0.4756
Epoch [21/50], Step [10/24], Loss: 0.3868
Epoch [21/50], Step [20/24], Loss: 0.3904
 42%|████▏     | 21/50 [1:08:42<1:32:58, 192.35s/it]Pixel accuracy: 0.8546, Mean IoU: 0.2793, Frequency weighted IoU: 0.7531, Loss: 0.5468
Epoch [22/50], Step [10/24], Loss: 0.3741
Epoch [22/50], Step [20/24], Loss: 0.3959
 44%|████▍     | 22/50 [1:11:53<1:29:34, 191.95s/it]Pixel accuracy: 0.8674, Mean IoU: 0.3127, Frequency weighted IoU: 0.7775, Loss: 0.4623
Epoch [23/50], Step [10/24], Loss: 0.3601
Epoch [23/50], Step [20/24], Loss: 0.3676
 46%|████▌     | 23/50 [1:15:07<1:26:40, 192.60s/it]Pixel accuracy: 0.8553, Mean IoU: 0.2979, Frequency weighted IoU: 0.7687, Loss: 0.4753
Epoch [24/50], Step [10/24], Loss: 0.3721
Epoch [24/50], Step [20/24], Loss: 0.3543
 48%|████▊     | 24/50 [1:18:22<1:23:46, 193.34s/it]Pixel accuracy: 0.8496, Mean IoU: 0.3044, Frequency weighted IoU: 0.7682, Loss: 0.4911
Epoch [25/50], Step [10/24], Loss: 0.3876
Epoch [25/50], Step [20/24], Loss: 0.3445
 50%|█████     | 25/50 [1:21:32<1:20:04, 192.17s/it]Pixel accuracy: 0.8026, Mean IoU: 0.2815, Frequency weighted IoU: 0.7220, Loss: 0.6582
Epoch [26/50], Step [10/24], Loss: 0.4112
Epoch [26/50], Step [20/24], Loss: 0.3893
 52%|█████▏    | 26/50 [1:24:45<1:16:59, 192.50s/it]Pixel accuracy: 0.8678, Mean IoU: 0.2944, Frequency weighted IoU: 0.7738, Loss: 0.4621
Epoch [27/50], Step [10/24], Loss: 0.3635
Epoch [27/50], Step [20/24], Loss: 0.3353
 54%|█████▍    | 27/50 [1:27:59<1:13:58, 192.98s/it]Pixel accuracy: 0.8714, Mean IoU: 0.3319, Frequency weighted IoU: 0.7859, Loss: 0.4302
Epoch [28/50], Step [10/24], Loss: 0.3377
Epoch [28/50], Step [20/24], Loss: 0.3572
 56%|█████▌    | 28/50 [1:31:15<1:11:01, 193.72s/it]Pixel accuracy: 0.8738, Mean IoU: 0.3149, Frequency weighted IoU: 0.7864, Loss: 0.4475
Epoch [29/50], Step [10/24], Loss: 0.3455
Epoch [29/50], Step [20/24], Loss: 0.3331
 58%|█████▊    | 29/50 [1:34:28<1:07:45, 193.61s/it]Pixel accuracy: 0.8745, Mean IoU: 0.3433, Frequency weighted IoU: 0.7941, Loss: 0.4124
Epoch [30/50], Step [10/24], Loss: 0.3121
Epoch [30/50], Step [20/24], Loss: 0.3073
 60%|██████    | 30/50 [1:37:44<1:04:44, 194.24s/it]Pixel accuracy: 0.8800, Mean IoU: 0.3440, Frequency weighted IoU: 0.7962, Loss: 0.4011
Epoch [31/50], Step [10/24], Loss: 0.3091
Epoch [31/50], Step [20/24], Loss: 0.2929
 62%|██████▏   | 31/50 [1:40:59<1:01:36, 194.58s/it]Pixel accuracy: 0.8773, Mean IoU: 0.3445, Frequency weighted IoU: 0.7936, Loss: 0.4120
Epoch [32/50], Step [10/24], Loss: 0.3058
Epoch [32/50], Step [20/24], Loss: 0.3060
 64%|██████▍   | 32/50 [1:44:15<58:31, 195.07s/it]  Pixel accuracy: 0.8563, Mean IoU: 0.3217, Frequency weighted IoU: 0.7798, Loss: 0.4630
Epoch [33/50], Step [10/24], Loss: 0.3251
Epoch [33/50], Step [20/24], Loss: 0.3269
 66%|██████▌   | 33/50 [1:47:30<55:16, 195.09s/it]Pixel accuracy: 0.8783, Mean IoU: 0.3545, Frequency weighted IoU: 0.7981, Loss: 0.4045
Epoch [34/50], Step [10/24], Loss: 0.2990
Epoch [34/50], Step [20/24], Loss: 0.2902
 68%|██████▊   | 34/50 [1:50:46<52:03, 195.23s/it]Pixel accuracy: 0.8784, Mean IoU: 0.3713, Frequency weighted IoU: 0.7979, Loss: 0.3939
Epoch [35/50], Step [10/24], Loss: 0.3022
Epoch [35/50], Step [20/24], Loss: 0.2884
 70%|███████   | 35/50 [1:54:00<48:41, 194.78s/it]Pixel accuracy: 0.8716, Mean IoU: 0.3611, Frequency weighted IoU: 0.7937, Loss: 0.4160
Epoch [36/50], Step [10/24], Loss: 0.3084
Epoch [36/50], Step [20/24], Loss: 0.3080
 72%|███████▏  | 36/50 [1:57:12<45:16, 194.01s/it]Pixel accuracy: 0.8629, Mean IoU: 0.3418, Frequency weighted IoU: 0.7793, Loss: 0.4585
Epoch [37/50], Step [10/24], Loss: 0.3260
Epoch [37/50], Step [20/24], Loss: 0.3009
 74%|███████▍  | 37/50 [2:00:25<41:57, 193.67s/it]Pixel accuracy: 0.8804, Mean IoU: 0.3474, Frequency weighted IoU: 0.7969, Loss: 0.4095
Epoch [38/50], Step [10/24], Loss: 0.2856
Epoch [38/50], Step [20/24], Loss: 0.2982
 76%|███████▌  | 38/50 [2:03:39<38:47, 193.93s/it]Pixel accuracy: 0.8881, Mean IoU: 0.3867, Frequency weighted IoU: 0.8110, Loss: 0.3613
Epoch [39/50], Step [10/24], Loss: 0.2540
Epoch [39/50], Step [20/24], Loss: 0.2670
 78%|███████▊  | 39/50 [2:06:56<35:43, 194.83s/it]Pixel accuracy: 0.8887, Mean IoU: 0.3845, Frequency weighted IoU: 0.8120, Loss: 0.3644
Epoch [40/50], Step [10/24], Loss: 0.2565
Epoch [40/50], Step [20/24], Loss: 0.2477
 80%|████████  | 40/50 [2:10:10<32:25, 194.59s/it]Pixel accuracy: 0.8888, Mean IoU: 0.3836, Frequency weighted IoU: 0.8121, Loss: 0.3624
Epoch [41/50], Step [10/24], Loss: 0.2579
Epoch [41/50], Step [20/24], Loss: 0.2566
 82%|████████▏ | 41/50 [2:13:21<29:01, 193.54s/it]Pixel accuracy: 0.8844, Mean IoU: 0.3839, Frequency weighted IoU: 0.8048, Loss: 0.3802
Epoch [42/50], Step [10/24], Loss: 0.2507
Epoch [42/50], Step [20/24], Loss: 0.2585
 84%|████████▍ | 42/50 [2:16:32<25:40, 192.60s/it]Pixel accuracy: 0.8889, Mean IoU: 0.4070, Frequency weighted IoU: 0.8159, Loss: 0.3611
Epoch [43/50], Step [10/24], Loss: 0.2630
Epoch [43/50], Step [20/24], Loss: 0.2570
 86%|████████▌ | 43/50 [2:19:44<22:27, 192.55s/it]Pixel accuracy: 0.8863, Mean IoU: 0.3938, Frequency weighted IoU: 0.8117, Loss: 0.3759
Epoch [44/50], Step [10/24], Loss: 0.2651
Epoch [44/50], Step [20/24], Loss: 0.2521
 88%|████████▊ | 44/50 [2:22:58<19:16, 192.76s/it]Pixel accuracy: 0.8895, Mean IoU: 0.3925, Frequency weighted IoU: 0.8152, Loss: 0.3633
Epoch [45/50], Step [10/24], Loss: 0.2384
Epoch [45/50], Step [20/24], Loss: 0.2450
 90%|█████████ | 45/50 [2:26:10<16:03, 192.61s/it]Pixel accuracy: 0.8938, Mean IoU: 0.4060, Frequency weighted IoU: 0.8205, Loss: 0.3462
Epoch [46/50], Step [10/24], Loss: 0.2355
Epoch [46/50], Step [20/24], Loss: 0.2470
 92%|█████████▏| 46/50 [2:29:27<12:56, 194.02s/it]Pixel accuracy: 0.8933, Mean IoU: 0.4076, Frequency weighted IoU: 0.8218, Loss: 0.3474
Epoch [47/50], Step [10/24], Loss: 0.2467
Epoch [47/50], Step [20/24], Loss: 0.2160
 94%|█████████▍| 47/50 [2:32:40<09:41, 193.79s/it]Pixel accuracy: 0.8900, Mean IoU: 0.3956, Frequency weighted IoU: 0.8178, Loss: 0.3577
Epoch [48/50], Step [10/24], Loss: 0.2466
Epoch [48/50], Step [20/24], Loss: 0.2205
 96%|█████████▌| 48/50 [2:35:52<06:26, 193.30s/it]Pixel accuracy: 0.8959, Mean IoU: 0.4152, Frequency weighted IoU: 0.8220, Loss: 0.3376
Epoch [49/50], Step [10/24], Loss: 0.2288
Epoch [49/50], Step [20/24], Loss: 0.2320
 98%|█████████▊| 49/50 [2:39:06<03:13, 193.48s/it]Pixel accuracy: 0.8951, Mean IoU: 0.4102, Frequency weighted IoU: 0.8188, Loss: 0.3458
Epoch [50/50], Step [10/24], Loss: 0.2304
Epoch [50/50], Step [20/24], Loss: 0.2455
100%|██████████| 50/50 [2:42:17<00:00, 194.75s/it]Pixel accuracy: 0.8920, Mean IoU: 0.3987, Frequency weighted IoU: 0.8191, Loss: 0.3648
====================
Finished Training, evaluating the model on the test set

Pixel accuracy: 0.8622, Mean IoU: 0.3320, Frequency weighted IoU: 0.7735, Loss: 0.4829
====================
Visualizing the model on the test set, the results will be saved in the vis/ directory
100%|██████████| 231/231 [01:39<00:00,  2.31it/s]



Team Information= No team, completed it without any partners

answers to questions in part1=

1)

The task of classification in images refers to the recognition of “things” (e.g., objects) and “stuff” (e.g., background) on a whole-image basis or on a subimage basis (i.e., bounding box, object detection, part and key-point detection, local correspondence). The task of (semantic) segmentation refers to grouping of pixels into regions such that pixels in each region have the same “stuff class”. 
One potential conflict arises when the output of a classification model differs from the segmentation result. For example, a classification model might correctly identify a cat in an image, but the segmentation model may fail to precisely outline the cat's boundaries.


2)

FCN can cope with these conflicts because they are trained end-to-end for pixelwise prediction and from supervised pre-training. 
By treating semantic segmentation as a dense prediction problem, FCNs can capture detailed spatial information and produce pixel-level predictions, which helps to solve the conflicts between classification and segmentation tasks.
They defined a new FCN architecture that combines layers of the feature hierarchy and refines the spatial precision of the output. They created different versions of FCNs by adding skips that combine the final prediction layer with lower layers with finer strides. The reason they can have lower layers is because as they see fewer pixels, the finer scale predictions need fewer layers. They introduced three versions of FCNs. Each FCN has a different output stride: FCN-32, FCN-16, FCN-8 (as shown in Figure 3). 
These different versions of FCNs balance the trade-off between computational efficiency and prediction accuracy by varying the depth of the network and incorporating skip connections to preserve spatial information. FCN-32s prioritizes computational efficiency but may sacrifice fine details, while FCN-8s achieves the highest spatial resolution at the expense of increased computational complexity. 



3)

Pixel accuracy gives the ratio of correctly classified pixels in the entire dataset. So it can be found as (correctly classified pixels)/(total number of pixels). On the other hand, IU (intersection over union), also given as a jaccard index in lecture notes, measures the spatial overlap between predicted and ground truth segmentation masks. IU gives more holistic evaluation for segmentation tasks than pixel accuracy since IU considers true positive and false positive predictions.
Mean-IU calculates the average IU across all classes in the dataset. Mean-IU is useful for evaluating the segmentation model's generalization across different object categories. On the other hand, frequency- weighted IU weights the IU of each class by its frequency in the dataset. In other words, classes with higher occurrence contribute more to the overall fwIU score.



4)

The low spatial resolution in the final segmentation output was one of the limitations of FCN-32. This is caused due to the downsampling operations which reduces the spatial dimensions of feature maps as they propagate through the network layers. To cope with this problem, they developed something called skip connections that combines the feature maps from earlier layers with higher resolution ones from later layers. For example, incorporating multiple skip connections in FCN-8, enables it to have a decent spatial resolution. 
However, as can be seen in figure 4 and 6, there are still problems with fine details and handling objects at different scales. So, it struggles to handle complex details of individual objects, as this problem is very apparent in the boat example in figure 6. 
One potential direction that I can give for the improvement of this problem is experimenting with using pyramid pooling. This is because this method incorporates features at multiple scales. This could allow FCN to capture contextual information across different levels of detail. And this could help the improvement of the network's ability to handle objects at various scales and enhance spatial resolution in the final segmentation output.




